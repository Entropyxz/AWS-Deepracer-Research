# AWS DeepRacer Hyperparameter Optimization Research
Undergraduate research project (2024) analyzing the impact of hyperparameters (batch size, learning rate, gamma, epsilon) on AWS DeepRacer reinforcement learning models. Key findings: optimal settings improved completion rates and mean rewards per episode. Conducted during Texas State University REU, supported by NSF Grant CNS-2149950.
- Tools: Python, AWS (SageMaker, S3, DeepRacer), Docker, Jupyter Notebook, Linux (SSH remote server for training)
